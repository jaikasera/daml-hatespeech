{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sklearn \n",
    "import tensorflow as tf \n",
    "import nltk\n",
    "import gensim \n",
    "import json\n",
    "import keras_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TO-DO: Read dataset.json as a dictionary object \n",
    "with open('dataset.json', 'r') as file:\n",
    "    json_str = file.read()\n",
    "    json_dic = json.loads(json_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>annotators</th>\n",
       "      <th>rationales</th>\n",
       "      <th>post_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1179055004553900032_twitter</th>\n",
       "      <td>1179055004553900032_twitter</td>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 1, 'targe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>i dont think im getting my baby them white 9 h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179063826874032128_twitter</th>\n",
       "      <td>1179063826874032128_twitter</td>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 1, 'targe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>we cannot continue calling ourselves feminists...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178793830532956161_twitter</th>\n",
       "      <td>1178793830532956161_twitter</td>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 4, 'targe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>nawt yall niggers ignoring me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179088797964763136_twitter</th>\n",
       "      <td>1179088797964763136_twitter</td>\n",
       "      <td>[{'label': 'hatespeech', 'annotator_id': 1, 't...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>&lt;user&gt; i am bit confused coz chinese ppl can n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179085312976445440_twitter</th>\n",
       "      <td>1179085312976445440_twitter</td>\n",
       "      <td>[{'label': 'hatespeech', 'annotator_id': 4, 't...</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>this bitch in whataburger eating a burger with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 post_id  \\\n",
       "1179055004553900032_twitter  1179055004553900032_twitter   \n",
       "1179063826874032128_twitter  1179063826874032128_twitter   \n",
       "1178793830532956161_twitter  1178793830532956161_twitter   \n",
       "1179088797964763136_twitter  1179088797964763136_twitter   \n",
       "1179085312976445440_twitter  1179085312976445440_twitter   \n",
       "\n",
       "                                                                    annotators  \\\n",
       "1179055004553900032_twitter  [{'label': 'normal', 'annotator_id': 1, 'targe...   \n",
       "1179063826874032128_twitter  [{'label': 'normal', 'annotator_id': 1, 'targe...   \n",
       "1178793830532956161_twitter  [{'label': 'normal', 'annotator_id': 4, 'targe...   \n",
       "1179088797964763136_twitter  [{'label': 'hatespeech', 'annotator_id': 1, 't...   \n",
       "1179085312976445440_twitter  [{'label': 'hatespeech', 'annotator_id': 4, 't...   \n",
       "\n",
       "                                                                    rationales  \\\n",
       "1179055004553900032_twitter                                                 []   \n",
       "1179063826874032128_twitter                                                 []   \n",
       "1178793830532956161_twitter                                                 []   \n",
       "1179088797964763136_twitter  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1179085312976445440_twitter  [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                                                   post_tokens  \n",
       "1179055004553900032_twitter  i dont think im getting my baby them white 9 h...  \n",
       "1179063826874032128_twitter  we cannot continue calling ourselves feminists...  \n",
       "1178793830532956161_twitter                      nawt yall niggers ignoring me  \n",
       "1179088797964763136_twitter  <user> i am bit confused coz chinese ppl can n...  \n",
       "1179085312976445440_twitter  this bitch in whataburger eating a burger with...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TO-DO: Convert dictionary object to pandas dataframe \n",
    "df = pd.DataFrame.from_dict(json_dic)\n",
    "def join_tokens(tokens):\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['post_tokens'] = df['post_tokens'].apply(join_tokens)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def clean_text(text_features):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "   \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    clean_text = []\n",
    "\n",
    "    for text in text_features:\n",
    "        words = word_tokenize(text)\n",
    "\n",
    "        clean_words = [lemmatizer.lemmatize(word) for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    pre-process text features of our dataset:  \n",
    "    - Case Lowering\n",
    "    - Tokenization\n",
    "    - Stop Word Removal\n",
    "    - Any other you want, including TF-IDF filtering \n",
    "    \n",
    "    Paramaters\n",
    "    ---------\n",
    "    text_features: text (tweet) data in our dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    ##TO-DO: complete this function!\n",
    "    ##When determining how to return the processed data, consider the format your data needs to be in for train_test_split and model training\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "def vectorize_text(cleaned_text, use_word2vec=True):\n",
    "    \"\"\"\n",
    "    use Word2Vec or Sentence2Vec to vectorize the cleaned text data\n",
    " \n",
    "    Paramaters\n",
    "    ---------\n",
    "    cleaned_text: (cleaned) text (tweet) data in our dataframe.\n",
    "    \"\"\"\n",
    "    ##TO-DO: complete this function!\n",
    "    ##When determining how to return the processed data, consider the format your data needs to be in for train_test_split and model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training: KNN/Random Forest/RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## TO-DO: split our data into train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using KNN\n",
    "###### Refer to this resource: https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: Determine initial value of parameters (especially K)\n",
    "k = 0\n",
    "\n",
    "## TO-DO: determine the best distance computation metric\n",
    "candidate_metrics = ['euclidean','cosine','manhattan']\n",
    "best_metric = ''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "## TO-DO: fit a KNN Classifier with distance metric and K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO: calculating accuracy\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: optimize K with hyperparameter tuning\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "## TO-DO: Visualize results to determine best K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: repeat training with optimized K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using XGBoost/Random Forest\n",
    "###### Refer to this resource: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: Train model naively, with default parameters\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: Evaluate base model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TO-DO: Hyperparameter Tuning via RandomizedSearch & GridSearch \n",
    "\n",
    "## get current parameters and their values \n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "## Randomized Search \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "random_grid = {}\n",
    "rf_random = RandomizedSearchCV()\n",
    "\n",
    "## Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {}\n",
    "grid_search = GridSearchCV()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using LSTMs\n",
    "###### Refer to this resource: https://www.youtube.com/watch?v=8HyCNIVRbSU&ab_channel=TheA.I.Hacker-MichaelPhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import LSTM, Activation, Dropout, Dense, Embedding\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from keras_preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## veryyyyyy basic architecture. TO-DO: complete and expand\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding())\n",
    "model.add(LSTM())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='',optimizer='', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 0\n",
    "batch_size = 0\n",
    "\n",
    "## TO-DO: determine training parameters and complete call\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: evaluate function \n",
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BERT\n",
    "###### Refer to: http://jalammar.github.io/illustrated-bert/ \n",
    "###### Refer to: https://medium.com/@parthdholakiya180/twitter-hate-detection-using-bert-e7682b2d0a0c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AITP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
