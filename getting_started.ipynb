{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sklearn \n",
    "import tensorflow as tf \n",
    "import nltk\n",
    "import gensim\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TO-DO: Read dataset.json as a dictionary object \n",
    "with open('dataset.json', 'r') as file:\n",
    "    json_str = file.read()\n",
    "    json_dic = json.loads(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotators</th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_tokens</th>\n",
       "      <th>rationales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1179055004553900032_twitter</th>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 1, 'targe...</td>\n",
       "      <td>1179055004553900032_twitter</td>\n",
       "      <td>i dont think im getting my baby them white 9 h...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179063826874032128_twitter</th>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 1, 'targe...</td>\n",
       "      <td>1179063826874032128_twitter</td>\n",
       "      <td>we cannot continue calling ourselves feminists...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178793830532956161_twitter</th>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 4, 'targe...</td>\n",
       "      <td>1178793830532956161_twitter</td>\n",
       "      <td>nawt yall niggers ignoring me</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179088797964763136_twitter</th>\n",
       "      <td>[{'label': 'hatespeech', 'annotator_id': 1, 't...</td>\n",
       "      <td>1179088797964763136_twitter</td>\n",
       "      <td>&lt;user&gt; i am bit confused coz chinese ppl can n...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179085312976445440_twitter</th>\n",
       "      <td>[{'label': 'hatespeech', 'annotator_id': 4, 't...</td>\n",
       "      <td>1179085312976445440_twitter</td>\n",
       "      <td>this bitch in whataburger eating a burger with...</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    annotators  \\\n",
       "1179055004553900032_twitter  [{'label': 'normal', 'annotator_id': 1, 'targe...   \n",
       "1179063826874032128_twitter  [{'label': 'normal', 'annotator_id': 1, 'targe...   \n",
       "1178793830532956161_twitter  [{'label': 'normal', 'annotator_id': 4, 'targe...   \n",
       "1179088797964763136_twitter  [{'label': 'hatespeech', 'annotator_id': 1, 't...   \n",
       "1179085312976445440_twitter  [{'label': 'hatespeech', 'annotator_id': 4, 't...   \n",
       "\n",
       "                                                 post_id  \\\n",
       "1179055004553900032_twitter  1179055004553900032_twitter   \n",
       "1179063826874032128_twitter  1179063826874032128_twitter   \n",
       "1178793830532956161_twitter  1178793830532956161_twitter   \n",
       "1179088797964763136_twitter  1179088797964763136_twitter   \n",
       "1179085312976445440_twitter  1179085312976445440_twitter   \n",
       "\n",
       "                                                                   post_tokens  \\\n",
       "1179055004553900032_twitter  i dont think im getting my baby them white 9 h...   \n",
       "1179063826874032128_twitter  we cannot continue calling ourselves feminists...   \n",
       "1178793830532956161_twitter                      nawt yall niggers ignoring me   \n",
       "1179088797964763136_twitter  <user> i am bit confused coz chinese ppl can n...   \n",
       "1179085312976445440_twitter  this bitch in whataburger eating a burger with...   \n",
       "\n",
       "                                                                    rationales  \n",
       "1179055004553900032_twitter                                                 []  \n",
       "1179063826874032128_twitter                                                 []  \n",
       "1178793830532956161_twitter                                                 []  \n",
       "1179088797964763136_twitter  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "1179085312976445440_twitter  [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TO-DO: Convert dictionary object to pandas dataframe \n",
    "df = pd.DataFrame.from_dict(json_dic).T\n",
    "for i in range (len(df.index)):\n",
    "    df['post_tokens'].iloc[i] = \" \".join(df['post_tokens'].iloc[i])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/23brianc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1179055004553900032_twitter    [i, dont, think, im, getting, my, baby, them, ...\n",
      "1179063826874032128_twitter    [we, can, not, continue, calling, ourselves, f...\n",
      "1178793830532956161_twitter                  [nawt, yall, niggers, ignoring, me]\n",
      "1179088797964763136_twitter    [<, user, >, i, am, bit, confused, coz, chines...\n",
      "1179085312976445440_twitter    [this, bitch, in, whataburger, eating, a, burg...\n",
      "13851720_gab                   [laura, loomer, raped, me, while, screaming, a...\n",
      "24198545_gab                   [and, this, is, why, i, end, up, with, nigger,...\n",
      "14567516_gab                            [nogs, jews, and, dykes, how, enriching]\n",
      "19679648_gab                   [guilty, until, proven, innocent, unless, you,...\n",
      "15858421_gab                   [tired, i, can, not, support, abortion, from, ...\n",
      "23844035_gab                   [<, number, >, <, number, >, <, percent, >, of...\n",
      "1089312238102609921_twitter    [tariq, nasheed, beating, the, fuck, out, of, ...\n",
      "1159278048069464065_twitter    [<, percent, >, cotton, thigh, highs, to, avoi...\n",
      "1170440965888335874_twitter    [oomf, a, <, number, >, yearl, old, closet, ga...\n",
      "1104904416430194688_twitter    [all, of, my, exes, were, cute, but, they, wer...\n",
      "1122734457230774272_twitter    [fellas, be, careful, she, texting, other, nig...\n",
      "1084835380619300864_twitter    [italian, niggas, call, they, dick, the, poozi...\n",
      "1178818409812746240_twitter    [<, user, >, what, did, the, old, lady, do, wa...\n",
      "1178905338386567168_twitter    [<, user, >, <, user, >, <, user, >, <, user, ...\n",
      "18696652_gab                   [<, user, >, condoning, drug, use, not, kike, ...\n",
      "1178692320201527307_twitter    [<, user, >, <, user, >, people, act, as, if, ...\n",
      "1178694945319858184_twitter    [<, user, >, <, user, >, they, visited, helped...\n",
      "1178177759971086336_twitter    [anyways, the, fact, that, her, outfit, direct...\n",
      "15544666_gab                                        [lol, what, a, stupid, kike]\n",
      "26031285_gab                   [it, funny, how, people, offended, by, the, wo...\n",
      "19346774_gab                   [as, much, as, i, appreciate, a, format, of, o...\n",
      "14104384_gab                   [why, does, anyone, care, what, the, albino, n...\n",
      "15284640_gab                   [that, was, just, a, photo, look, up, rhyan, m...\n",
      "12533510_gab                   [this, sounds, like, a, kike, national, social...\n",
      "19897606_gab                   [i, saw, a, couple, baums, and, steins, in, th...\n",
      "                                                     ...                        \n",
      "9837326_gab                    [so, much, anti, white, rhetoric, is, out, of,...\n",
      "9841810_gab                    [jews, couch, their, distrust, of, whites, as,...\n",
      "9842531_gab                    [i, disagree, before, the, british, started, t...\n",
      "9844016_gab                    [gas, all, niggers, jews, the, world, over, pr...\n",
      "9844694_gab                    [this, is, pretty, much, the, conclusion, of, ...\n",
      "9845733_gab                    [do, not, for, get, my, followers, the, jews, ...\n",
      "9849931_gab                    [sure, it, fake, religion, like, everything, f...\n",
      "9853994_gab                    [define, fair, a, trial, with, of, the, jury, ...\n",
      "9855482_gab                    [insidious, evil, of, the, united, nations, an...\n",
      "9866948_gab                    [trump, making, america, great, again, after, ...\n",
      "9867117_gab                    [well, my, dear, lgbtq, brothers, and, sisters...\n",
      "9867294_gab                    [britain, and, usa, are, israel, real, enemies...\n",
      "9869181_gab                    [but, it, on, hbo, more, violence, and, sex, c...\n",
      "9869480_gab                    [it, a, bitch, to, find, i, accidently, opened...\n",
      "9870025_gab                    [anybody, notice, anything, strange, on, gab, ...\n",
      "9872639_gab                    [graph, straftaten, gegen, die, sexuelle, selb...\n",
      "9875482_gab                    [c, mon, men, charge, it, for, the, gays, tran...\n",
      "9877889_gab                    [just, because, there, socialist, in, the, bra...\n",
      "9878150_gab                    [an, afghani, immigrant, once, told, me, that,...\n",
      "9976594_gab                    [<, number, >, out, of, <, number, >, parole, ...\n",
      "9981407_gab                    [<, user, >, congrats, on, making, the, kike, ...\n",
      "9981575_gab                    [wow, she, comes, right, out, and, says, the, ...\n",
      "9982674_gab                    [more, and, more, leftists, are, moving, close...\n",
      "9984358_gab                                              [a, negro, honey, trap]\n",
      "9988840_gab                    [this, vile, racist, ghetto, ass, dumber, than...\n",
      "9989999_gab                    [if, ur, still, on, twitter, tell, carlton, i,...\n",
      "9990225_gab                    [when, i, first, got, on, here, and, said, i, ...\n",
      "9991681_gab                    [was, macht, der, moslem, wenn, der, zion, geg...\n",
      "9992513_gab                    [it, is, awful, look, at, world, demographics,...\n",
      "9998729_gab                    [the, jewish, globalist, elite, have, only, im...\n",
      "Name: tokenized, Length: 20148, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "df['tokenized'] = df.apply(lambda row: nltk.word_tokenize(row['post_tokens']), axis=1)\n",
    "\n",
    "print(df['tokenized'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/23brianc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/23brianc/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "def clean_text(text_features):\n",
    "    \n",
    "    text_features = text_features.str.lower()\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # df['tweet_without_stopword'] = text_features.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "    df['tweet_without_stopword'] = text_features.apply(lambda x:[word for word in x.split() if word not in (stop_words)])\n",
    "\n",
    "    tweets_without_stopword = list(df['tweet_without_stopword']) \n",
    "\n",
    "    lemmatized = []\n",
    "    \n",
    "    for words in tweets_without_stopword:\n",
    "        temp = []\n",
    "\n",
    "        for w in words:\n",
    "            temp.append(lemmatizer.lemmatize(w))\n",
    "            \n",
    "        lemmatized.append(temp)     \n",
    "        \n",
    "\n",
    "    df['text_lemmatized'] = lemmatized\n",
    "    \n",
    "    \"\"\"\n",
    "    pre-process text features of our dataset:  \n",
    "    - Case Lowering\n",
    "    - Tokenization\n",
    "    - Stop Word Removal\n",
    "    - Any other you want, including TF-IDF filtering \n",
    " \n",
    "    Paramaters\n",
    "    ---------\n",
    "    text_features: text (tweet) data in our dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    ##TO-DO: complete this function!\n",
    "    ##When determining how to return the processed data, consider the format your data needs to be in for train_test_split and model training\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotators</th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_tokens</th>\n",
       "      <th>rationales</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tweet_without_stopword</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1179055004553900032_twitter</th>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 1, 'targe...</td>\n",
       "      <td>1179055004553900032_twitter</td>\n",
       "      <td>i dont think im getting my baby them white 9 h...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[i, dont, think, im, getting, my, baby, them, ...</td>\n",
       "      <td>dont think im getting baby white 9 two white j...</td>\n",
       "      <td>[d, o, n, t,  , t, h, i, n, k,  , i, m,  , g, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179063826874032128_twitter</th>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 1, 'targe...</td>\n",
       "      <td>1179063826874032128_twitter</td>\n",
       "      <td>we cannot continue calling ourselves feminists...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[we, can, not, continue, calling, ourselves, f...</td>\n",
       "      <td>cannot continue calling feminists rights womxn...</td>\n",
       "      <td>[c, a, n, n, o, t,  , c, o, n, t, i, n, u, e, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178793830532956161_twitter</th>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 4, 'targe...</td>\n",
       "      <td>1178793830532956161_twitter</td>\n",
       "      <td>nawt yall niggers ignoring me</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nawt, yall, niggers, ignoring, me]</td>\n",
       "      <td>nawt yall niggers ignoring</td>\n",
       "      <td>[n, a, w, t,  , y, a, l, l,  , n, i, g, g, e, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179088797964763136_twitter</th>\n",
       "      <td>[{'label': 'hatespeech', 'annotator_id': 1, 't...</td>\n",
       "      <td>1179088797964763136_twitter</td>\n",
       "      <td>&lt;user&gt; i am bit confused coz chinese ppl can n...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[&lt;, user, &gt;, i, am, bit, confused, coz, chines...</td>\n",
       "      <td>&lt;user&gt; bit confused coz chinese ppl access twi...</td>\n",
       "      <td>[&lt;, u, s, e, r, &gt;,  , b, i, t,  , c, o, n, f, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179085312976445440_twitter</th>\n",
       "      <td>[{'label': 'hatespeech', 'annotator_id': 4, 't...</td>\n",
       "      <td>1179085312976445440_twitter</td>\n",
       "      <td>this bitch in whataburger eating a burger with...</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[this, bitch, in, whataburger, eating, a, burg...</td>\n",
       "      <td>bitch whataburger eating burger top bun holdin...</td>\n",
       "      <td>[b, i, t, c, h,  , w, h, a, t, a, b, u, r, g, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    annotators  \\\n",
       "1179055004553900032_twitter  [{'label': 'normal', 'annotator_id': 1, 'targe...   \n",
       "1179063826874032128_twitter  [{'label': 'normal', 'annotator_id': 1, 'targe...   \n",
       "1178793830532956161_twitter  [{'label': 'normal', 'annotator_id': 4, 'targe...   \n",
       "1179088797964763136_twitter  [{'label': 'hatespeech', 'annotator_id': 1, 't...   \n",
       "1179085312976445440_twitter  [{'label': 'hatespeech', 'annotator_id': 4, 't...   \n",
       "\n",
       "                                                 post_id  \\\n",
       "1179055004553900032_twitter  1179055004553900032_twitter   \n",
       "1179063826874032128_twitter  1179063826874032128_twitter   \n",
       "1178793830532956161_twitter  1178793830532956161_twitter   \n",
       "1179088797964763136_twitter  1179088797964763136_twitter   \n",
       "1179085312976445440_twitter  1179085312976445440_twitter   \n",
       "\n",
       "                                                                   post_tokens  \\\n",
       "1179055004553900032_twitter  i dont think im getting my baby them white 9 h...   \n",
       "1179063826874032128_twitter  we cannot continue calling ourselves feminists...   \n",
       "1178793830532956161_twitter                      nawt yall niggers ignoring me   \n",
       "1179088797964763136_twitter  <user> i am bit confused coz chinese ppl can n...   \n",
       "1179085312976445440_twitter  this bitch in whataburger eating a burger with...   \n",
       "\n",
       "                                                                    rationales  \\\n",
       "1179055004553900032_twitter                                                 []   \n",
       "1179063826874032128_twitter                                                 []   \n",
       "1178793830532956161_twitter                                                 []   \n",
       "1179088797964763136_twitter  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1179085312976445440_twitter  [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                                                     tokenized  \\\n",
       "1179055004553900032_twitter  [i, dont, think, im, getting, my, baby, them, ...   \n",
       "1179063826874032128_twitter  [we, can, not, continue, calling, ourselves, f...   \n",
       "1178793830532956161_twitter                [nawt, yall, niggers, ignoring, me]   \n",
       "1179088797964763136_twitter  [<, user, >, i, am, bit, confused, coz, chines...   \n",
       "1179085312976445440_twitter  [this, bitch, in, whataburger, eating, a, burg...   \n",
       "\n",
       "                                                        tweet_without_stopword  \\\n",
       "1179055004553900032_twitter  dont think im getting baby white 9 two white j...   \n",
       "1179063826874032128_twitter  cannot continue calling feminists rights womxn...   \n",
       "1178793830532956161_twitter                         nawt yall niggers ignoring   \n",
       "1179088797964763136_twitter  <user> bit confused coz chinese ppl access twi...   \n",
       "1179085312976445440_twitter  bitch whataburger eating burger top bun holdin...   \n",
       "\n",
       "                                                               text_lemmatized  \n",
       "1179055004553900032_twitter  [d, o, n, t,  , t, h, i, n, k,  , i, m,  , g, ...  \n",
       "1179063826874032128_twitter  [c, a, n, n, o, t,  , c, o, n, t, i, n, u, e, ...  \n",
       "1178793830532956161_twitter  [n, a, w, t,  , y, a, l, l,  , n, i, g, g, e, ...  \n",
       "1179088797964763136_twitter  [<, u, s, e, r, >,  , b, i, t,  , c, o, n, f, ...  \n",
       "1179085312976445440_twitter  [b, i, t, c, h,  , w, h, a, t, a, b, u, r, g, ...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(df['post_tokens'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def vectorize_text(cleaned_text, use_word2vec=True):\n",
    "    \"\"\"\n",
    "    use Word2Vec or Sentence2Vec to vectorize the cleaned text data\n",
    " \n",
    "    Paramaters\n",
    "    ---------\n",
    "    cleaned_text: (cleaned) text (tweet) data in our dataframe.\n",
    "    \"\"\"\n",
    "    ##TO-DO: complete this function!\n",
    "    ##When determining how to return the processed data, consider the format your data needs to be in for train_test_split and model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training: KNN/Random Forest/RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## TO-DO: split our data into train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using KNN\n",
    "###### Refer to this resource: https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: Determine initial value of parameters (especially K)\n",
    "k = 0\n",
    "\n",
    "## TO-DO: determine the best distance computation metric\n",
    "candidate_metrics = ['euclidean','cosine','manhattan']\n",
    "best_metric = ''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "## TO-DO: fit a KNN Classifier with distance metric and K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO: calculating accuracy\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: optimize K with hyperparameter tuning\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "## TO-DO: Visualize results to determine best K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: repeat training with optimized K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using XGBoost/Random Forest\n",
    "###### Refer to this resource: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: Train model naively, with default parameters\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: Evaluate base model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TO-DO: Hyperparameter Tuning via RandomizedSearch & GridSearch \n",
    "\n",
    "## get current parameters and their values \n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "## Randomized Search \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "random_grid = {}\n",
    "rf_random = RandomizedSearchCV()\n",
    "\n",
    "## Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {}\n",
    "grid_search = GridSearchCV()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using LSTMs\n",
    "###### Refer to this resource: https://www.youtube.com/watch?v=8HyCNIVRbSU&ab_channel=TheA.I.Hacker-MichaelPhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import LSTM, Activation, Dropout, Dense, Embedding\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from keras_preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## veryyyyyy basic architecture. TO-DO: complete and expand\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding())\n",
    "model.add(LSTM())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='',optimizer='', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 0\n",
    "batch_size = 0\n",
    "\n",
    "## TO-DO: determine training parameters and complete call\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: evaluate function \n",
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BERT\n",
    "###### Refer to: http://jalammar.github.io/illustrated-bert/ \n",
    "###### Refer to: https://medium.com/@parthdholakiya180/twitter-hate-detection-using-bert-e7682b2d0a0c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
