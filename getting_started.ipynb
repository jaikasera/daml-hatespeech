{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sklearn \n",
    "import tensorflow as tf \n",
    "import nltk\n",
    "import gensim \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TO-DO: Read dataset.json as a dictionary object \n",
    "with open('dataset.json', 'r') as file:\n",
    "    json_str = file.read()\n",
    "    json_dic = json.loads(json_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>annotators</th>\n",
       "      <th>rationales</th>\n",
       "      <th>post_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1179055004553900032_twitter</th>\n",
       "      <td>1179055004553900032_twitter</td>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 1, 'targe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>i dont think im getting my baby them white 9 h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179063826874032128_twitter</th>\n",
       "      <td>1179063826874032128_twitter</td>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 1, 'targe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>we cannot continue calling ourselves feminists...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178793830532956161_twitter</th>\n",
       "      <td>1178793830532956161_twitter</td>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 4, 'targe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>nawt yall niggers ignoring me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179088797964763136_twitter</th>\n",
       "      <td>1179088797964763136_twitter</td>\n",
       "      <td>[{'label': 'hatespeech', 'annotator_id': 1, 't...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>&lt;user&gt; i am bit confused coz chinese ppl can n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179085312976445440_twitter</th>\n",
       "      <td>1179085312976445440_twitter</td>\n",
       "      <td>[{'label': 'hatespeech', 'annotator_id': 4, 't...</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>this bitch in whataburger eating a burger with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 post_id  \\\n",
       "1179055004553900032_twitter  1179055004553900032_twitter   \n",
       "1179063826874032128_twitter  1179063826874032128_twitter   \n",
       "1178793830532956161_twitter  1178793830532956161_twitter   \n",
       "1179088797964763136_twitter  1179088797964763136_twitter   \n",
       "1179085312976445440_twitter  1179085312976445440_twitter   \n",
       "\n",
       "                                                                    annotators  \\\n",
       "1179055004553900032_twitter  [{'label': 'normal', 'annotator_id': 1, 'targe...   \n",
       "1179063826874032128_twitter  [{'label': 'normal', 'annotator_id': 1, 'targe...   \n",
       "1178793830532956161_twitter  [{'label': 'normal', 'annotator_id': 4, 'targe...   \n",
       "1179088797964763136_twitter  [{'label': 'hatespeech', 'annotator_id': 1, 't...   \n",
       "1179085312976445440_twitter  [{'label': 'hatespeech', 'annotator_id': 4, 't...   \n",
       "\n",
       "                                                                    rationales  \\\n",
       "1179055004553900032_twitter                                                 []   \n",
       "1179063826874032128_twitter                                                 []   \n",
       "1178793830532956161_twitter                                                 []   \n",
       "1179088797964763136_twitter  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1179085312976445440_twitter  [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                                                   post_tokens  \n",
       "1179055004553900032_twitter  i dont think im getting my baby them white 9 h...  \n",
       "1179063826874032128_twitter  we cannot continue calling ourselves feminists...  \n",
       "1178793830532956161_twitter                      nawt yall niggers ignoring me  \n",
       "1179088797964763136_twitter  <user> i am bit confused coz chinese ppl can n...  \n",
       "1179085312976445440_twitter  this bitch in whataburger eating a burger with...  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TO-DO: Convert dictionary object to pandas dataframe \n",
    "df = pd.DataFrame.from_dict(json_dic).T\n",
    "df['post_tokens'] = df['post_tokens'].apply(lambda x: \" \".join(x))\n",
    "df['post_tokens']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jaikasera/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jaikasera/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jaikasera/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def clean_text(text_features): #text_features is a string of words, and we can then apply this function onto every row in the dataframe \n",
    "    \n",
    "    \n",
    "    tokens = word_tokenize(text_features.lower())\n",
    "    \n",
    "    tokens = [x for x in tokens if x not in stopwords]\n",
    "\n",
    "    tokens = [word for word in tokens if any(char.isalpha() for char in word)]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(x) for x in tokens]\n",
    "    \n",
    "    return tokens\n",
    "    \"\"\"\n",
    "    pre-process text features of our dataset:  \n",
    "    - Case Lowering\n",
    "    - Tokenization\n",
    "    - Stop Word Removal\n",
    "    - Any other you want, including TF-IDF filtering \n",
    "    \n",
    "    Paramaters\n",
    "    ---------\n",
    "    text_features: text (tweet) data in our dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    ##TO-DO: complete this function!\n",
    "    ##When determining how to return the processed data, consider the format your data needs to be in for train_test_split and model training\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>annotators</th>\n",
       "      <th>rationales</th>\n",
       "      <th>post_tokens</th>\n",
       "      <th>post_tokens_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1179055004553900032_twitter</th>\n",
       "      <td>1179055004553900032_twitter</td>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 1, 'targe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>i dont think im getting my baby them white 9 h...</td>\n",
       "      <td>[dont, think, im, getting, baby, white, two, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179063826874032128_twitter</th>\n",
       "      <td>1179063826874032128_twitter</td>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 1, 'targe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>we cannot continue calling ourselves feminists...</td>\n",
       "      <td>[continue, calling, feminist, right, womxn, ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178793830532956161_twitter</th>\n",
       "      <td>1178793830532956161_twitter</td>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 4, 'targe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>nawt yall niggers ignoring me</td>\n",
       "      <td>[nawt, yall, nigger, ignoring]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179088797964763136_twitter</th>\n",
       "      <td>1179088797964763136_twitter</td>\n",
       "      <td>[{'label': 'hatespeech', 'annotator_id': 1, 't...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>&lt;user&gt; i am bit confused coz chinese ppl can n...</td>\n",
       "      <td>[user, bit, confused, coz, chinese, ppl, acces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179085312976445440_twitter</th>\n",
       "      <td>1179085312976445440_twitter</td>\n",
       "      <td>[{'label': 'hatespeech', 'annotator_id': 4, 't...</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>this bitch in whataburger eating a burger with...</td>\n",
       "      <td>[bitch, whataburger, eating, burger, top, bun,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 post_id  \\\n",
       "1179055004553900032_twitter  1179055004553900032_twitter   \n",
       "1179063826874032128_twitter  1179063826874032128_twitter   \n",
       "1178793830532956161_twitter  1178793830532956161_twitter   \n",
       "1179088797964763136_twitter  1179088797964763136_twitter   \n",
       "1179085312976445440_twitter  1179085312976445440_twitter   \n",
       "\n",
       "                                                                    annotators  \\\n",
       "1179055004553900032_twitter  [{'label': 'normal', 'annotator_id': 1, 'targe...   \n",
       "1179063826874032128_twitter  [{'label': 'normal', 'annotator_id': 1, 'targe...   \n",
       "1178793830532956161_twitter  [{'label': 'normal', 'annotator_id': 4, 'targe...   \n",
       "1179088797964763136_twitter  [{'label': 'hatespeech', 'annotator_id': 1, 't...   \n",
       "1179085312976445440_twitter  [{'label': 'hatespeech', 'annotator_id': 4, 't...   \n",
       "\n",
       "                                                                    rationales  \\\n",
       "1179055004553900032_twitter                                                 []   \n",
       "1179063826874032128_twitter                                                 []   \n",
       "1178793830532956161_twitter                                                 []   \n",
       "1179088797964763136_twitter  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1179085312976445440_twitter  [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                                                   post_tokens  \\\n",
       "1179055004553900032_twitter  i dont think im getting my baby them white 9 h...   \n",
       "1179063826874032128_twitter  we cannot continue calling ourselves feminists...   \n",
       "1178793830532956161_twitter                      nawt yall niggers ignoring me   \n",
       "1179088797964763136_twitter  <user> i am bit confused coz chinese ppl can n...   \n",
       "1179085312976445440_twitter  this bitch in whataburger eating a burger with...   \n",
       "\n",
       "                                                           post_tokens_cleaned  \n",
       "1179055004553900032_twitter  [dont, think, im, getting, baby, white, two, w...  \n",
       "1179063826874032128_twitter  [continue, calling, feminist, right, womxn, ar...  \n",
       "1178793830532956161_twitter                     [nawt, yall, nigger, ignoring]  \n",
       "1179088797964763136_twitter  [user, bit, confused, coz, chinese, ppl, acces...  \n",
       "1179085312976445440_twitter  [bitch, whataburger, eating, burger, top, bun,...  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['post_tokens_cleaned'] = df['post_tokens'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(cleaned_text, use_word2vec=True):\n",
    "    \"\"\"\n",
    "    use Word2Vec or Sentence2Vec to vectorize the cleaned text data\n",
    " \n",
    "    Paramaters\n",
    "    ---------\n",
    "    cleaned_text: (cleaned) text (tweet) data in our dataframe.\n",
    "    \"\"\"\n",
    "    ##TO-DO: complete this function!\n",
    "    ##When determining how to return the processed data, consider the format your data needs to be in for train_test_split and model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training: KNN/Random Forest/RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## TO-DO: split our data into train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using KNN\n",
    "###### Refer to this resource: https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: Determine initial value of parameters (especially K)\n",
    "k = 0\n",
    "\n",
    "## TO-DO: determine the best distance computation metric\n",
    "candidate_metrics = ['euclidean','cosine','manhattan']\n",
    "best_metric = ''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "## TO-DO: fit a KNN Classifier with distance metric and K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO: calculating accuracy\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: optimize K with hyperparameter tuning\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "## TO-DO: Visualize results to determine best K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: repeat training with optimized K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using XGBoost/Random Forest\n",
    "###### Refer to this resource: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: Train model naively, with default parameters\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: Evaluate base model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RandomizedSearchCV.__init__() missing 2 required positional arguments: 'estimator' and 'param_distributions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[178], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomizedSearchCV\n\u001b[1;32m      9\u001b[0m random_grid \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> 10\u001b[0m rf_random \u001b[39m=\u001b[39m RandomizedSearchCV()\n\u001b[1;32m     12\u001b[0m \u001b[39m## Grid Search\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m GridSearchCV\n",
      "\u001b[0;31mTypeError\u001b[0m: RandomizedSearchCV.__init__() missing 2 required positional arguments: 'estimator' and 'param_distributions'"
     ]
    }
   ],
   "source": [
    "##TO-DO: Hyperparameter Tuning via RandomizedSearch & GridSearch \n",
    "\n",
    "## get current parameters and their values \n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "## Randomized Search \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "random_grid = {}\n",
    "rf_random = RandomizedSearchCV()\n",
    "\n",
    "## Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {}\n",
    "grid_search = GridSearchCV()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using LSTMs\n",
    "###### Refer to this resource: https://www.youtube.com/watch?v=8HyCNIVRbSU&ab_channel=TheA.I.Hacker-MichaelPhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import LSTM, Activation, Dropout, Dense, Embedding\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from keras_preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## veryyyyyy basic architecture. TO-DO: complete and expand\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding())\n",
    "model.add(LSTM())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='',optimizer='', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 0\n",
    "batch_size = 0\n",
    "\n",
    "## TO-DO: determine training parameters and complete call\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: evaluate function \n",
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BERT\n",
    "###### Refer to: http://jalammar.github.io/illustrated-bert/ \n",
    "###### Refer to: https://medium.com/@parthdholakiya180/twitter-hate-detection-using-bert-e7682b2d0a0c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AITP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
