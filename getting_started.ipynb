{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sklearn \n",
    "import tensorflow as tf \n",
    "import nltk\n",
    "import gensim \n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TO-DO: Read dataset.json as a dictionary object \n",
    "with open('dataset.json', 'r') as file:\n",
    "    json_str = file.read()\n",
    "    json_dic = json.loads(json_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>annotators</th>\n",
       "      <th>rationales</th>\n",
       "      <th>post_tokens</th>\n",
       "      <th>post_tokens_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1179055004553900032_twitter</th>\n",
       "      <td>1179055004553900032_twitter</td>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 1, 'targe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[i, dont, think, im, getting, my, baby, them, ...</td>\n",
       "      <td>i dont think im getting my baby them white 9 h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179063826874032128_twitter</th>\n",
       "      <td>1179063826874032128_twitter</td>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 1, 'targe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[we, cannot, continue, calling, ourselves, fem...</td>\n",
       "      <td>we cannot continue calling ourselves feminists...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178793830532956161_twitter</th>\n",
       "      <td>1178793830532956161_twitter</td>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 4, 'targe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nawt, yall, niggers, ignoring, me]</td>\n",
       "      <td>nawt yall niggers ignoring me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179088797964763136_twitter</th>\n",
       "      <td>1179088797964763136_twitter</td>\n",
       "      <td>[{'label': 'hatespeech', 'annotator_id': 1, 't...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[&lt;user&gt;, i, am, bit, confused, coz, chinese, p...</td>\n",
       "      <td>&lt;user&gt; i am bit confused coz chinese ppl can n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179085312976445440_twitter</th>\n",
       "      <td>1179085312976445440_twitter</td>\n",
       "      <td>[{'label': 'hatespeech', 'annotator_id': 4, 't...</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[this, bitch, in, whataburger, eating, a, burg...</td>\n",
       "      <td>this bitch in whataburger eating a burger with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 post_id  \\\n",
       "1179055004553900032_twitter  1179055004553900032_twitter   \n",
       "1179063826874032128_twitter  1179063826874032128_twitter   \n",
       "1178793830532956161_twitter  1178793830532956161_twitter   \n",
       "1179088797964763136_twitter  1179088797964763136_twitter   \n",
       "1179085312976445440_twitter  1179085312976445440_twitter   \n",
       "\n",
       "                                                                    annotators  \\\n",
       "1179055004553900032_twitter  [{'label': 'normal', 'annotator_id': 1, 'targe...   \n",
       "1179063826874032128_twitter  [{'label': 'normal', 'annotator_id': 1, 'targe...   \n",
       "1178793830532956161_twitter  [{'label': 'normal', 'annotator_id': 4, 'targe...   \n",
       "1179088797964763136_twitter  [{'label': 'hatespeech', 'annotator_id': 1, 't...   \n",
       "1179085312976445440_twitter  [{'label': 'hatespeech', 'annotator_id': 4, 't...   \n",
       "\n",
       "                                                                    rationales  \\\n",
       "1179055004553900032_twitter                                                 []   \n",
       "1179063826874032128_twitter                                                 []   \n",
       "1178793830532956161_twitter                                                 []   \n",
       "1179088797964763136_twitter  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1179085312976445440_twitter  [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                                                   post_tokens  \\\n",
       "1179055004553900032_twitter  [i, dont, think, im, getting, my, baby, them, ...   \n",
       "1179063826874032128_twitter  [we, cannot, continue, calling, ourselves, fem...   \n",
       "1178793830532956161_twitter                [nawt, yall, niggers, ignoring, me]   \n",
       "1179088797964763136_twitter  [<user>, i, am, bit, confused, coz, chinese, p...   \n",
       "1179085312976445440_twitter  [this, bitch, in, whataburger, eating, a, burg...   \n",
       "\n",
       "                                                            post_tokens_string  \n",
       "1179055004553900032_twitter  i dont think im getting my baby them white 9 h...  \n",
       "1179063826874032128_twitter  we cannot continue calling ourselves feminists...  \n",
       "1178793830532956161_twitter                      nawt yall niggers ignoring me  \n",
       "1179088797964763136_twitter  <user> i am bit confused coz chinese ppl can n...  \n",
       "1179085312976445440_twitter  this bitch in whataburger eating a burger with...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TO-DO: Convert dictionary object to pandas dataframe \n",
    "df = pd.DataFrame.from_dict(json_dic).T\n",
    "df['post_tokens_string'] = df['post_tokens'].apply(lambda x: ' '.join(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download()\n",
    "\n",
    "# download a create set of stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# exclude set contains punctuation\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# create lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#     # Paramaters: text_features: text (tweet) data in our dataframe.\n",
    "#     # When determining how to return the processed data, consider the format your data needs to be in for train_test_split and model training\n",
    " \n",
    "def clean_text(text_features):\n",
    "    # tokenize and lower case\n",
    "    tokens = word_tokenize(text_features.lower())\n",
    "\n",
    "    # remove stopwords\n",
    "    tokens = [x for x in tokens if x not in stopwords]\n",
    "\n",
    "    # remove punctuation \n",
    "    tokens = [token for token in tokens if token not in exclude]\n",
    "\n",
    "    # remove non alphabetical characters\n",
    "    tokens = [x for x in tokens if any(char.isalpha() for char in x)]\n",
    "\n",
    "    # undo contractions\n",
    "    tokens = [contractions.fix(token) for token in tokens]\n",
    "\n",
    "    # lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>annotators</th>\n",
       "      <th>rationales</th>\n",
       "      <th>post_tokens</th>\n",
       "      <th>post_tokens_string</th>\n",
       "      <th>post_tokens_cleaned</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1179055004553900032_twitter</th>\n",
       "      <td>1179055004553900032_twitter</td>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 1, 'targe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[i, dont, think, im, getting, my, baby, them, ...</td>\n",
       "      <td>i dont think im getting my baby them white 9 h...</td>\n",
       "      <td>[do not, think, i am, getting, baby, white, tw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179063826874032128_twitter</th>\n",
       "      <td>1179063826874032128_twitter</td>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 1, 'targe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[we, cannot, continue, calling, ourselves, fem...</td>\n",
       "      <td>we cannot continue calling ourselves feminists...</td>\n",
       "      <td>[continue, calling, feminist, right, womxn, ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178793830532956161_twitter</th>\n",
       "      <td>1178793830532956161_twitter</td>\n",
       "      <td>[{'label': 'normal', 'annotator_id': 4, 'targe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nawt, yall, niggers, ignoring, me]</td>\n",
       "      <td>nawt yall niggers ignoring me</td>\n",
       "      <td>[nawt, you all, nigger, ignoring]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179088797964763136_twitter</th>\n",
       "      <td>1179088797964763136_twitter</td>\n",
       "      <td>[{'label': 'hatespeech', 'annotator_id': 1, 't...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[&lt;user&gt;, i, am, bit, confused, coz, chinese, p...</td>\n",
       "      <td>&lt;user&gt; i am bit confused coz chinese ppl can n...</td>\n",
       "      <td>[user, bit, confused, coz, chinese, people, ac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179085312976445440_twitter</th>\n",
       "      <td>1179085312976445440_twitter</td>\n",
       "      <td>[{'label': 'hatespeech', 'annotator_id': 4, 't...</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[this, bitch, in, whataburger, eating, a, burg...</td>\n",
       "      <td>this bitch in whataburger eating a burger with...</td>\n",
       "      <td>[bitch, whataburger, eating, burger, top, bun,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 post_id  \\\n",
       "1179055004553900032_twitter  1179055004553900032_twitter   \n",
       "1179063826874032128_twitter  1179063826874032128_twitter   \n",
       "1178793830532956161_twitter  1178793830532956161_twitter   \n",
       "1179088797964763136_twitter  1179088797964763136_twitter   \n",
       "1179085312976445440_twitter  1179085312976445440_twitter   \n",
       "\n",
       "                                                                    annotators  \\\n",
       "1179055004553900032_twitter  [{'label': 'normal', 'annotator_id': 1, 'targe...   \n",
       "1179063826874032128_twitter  [{'label': 'normal', 'annotator_id': 1, 'targe...   \n",
       "1178793830532956161_twitter  [{'label': 'normal', 'annotator_id': 4, 'targe...   \n",
       "1179088797964763136_twitter  [{'label': 'hatespeech', 'annotator_id': 1, 't...   \n",
       "1179085312976445440_twitter  [{'label': 'hatespeech', 'annotator_id': 4, 't...   \n",
       "\n",
       "                                                                    rationales  \\\n",
       "1179055004553900032_twitter                                                 []   \n",
       "1179063826874032128_twitter                                                 []   \n",
       "1178793830532956161_twitter                                                 []   \n",
       "1179088797964763136_twitter  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1179085312976445440_twitter  [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                                                   post_tokens  \\\n",
       "1179055004553900032_twitter  [i, dont, think, im, getting, my, baby, them, ...   \n",
       "1179063826874032128_twitter  [we, cannot, continue, calling, ourselves, fem...   \n",
       "1178793830532956161_twitter                [nawt, yall, niggers, ignoring, me]   \n",
       "1179088797964763136_twitter  [<user>, i, am, bit, confused, coz, chinese, p...   \n",
       "1179085312976445440_twitter  [this, bitch, in, whataburger, eating, a, burg...   \n",
       "\n",
       "                                                            post_tokens_string  \\\n",
       "1179055004553900032_twitter  i dont think im getting my baby them white 9 h...   \n",
       "1179063826874032128_twitter  we cannot continue calling ourselves feminists...   \n",
       "1178793830532956161_twitter                      nawt yall niggers ignoring me   \n",
       "1179088797964763136_twitter  <user> i am bit confused coz chinese ppl can n...   \n",
       "1179085312976445440_twitter  this bitch in whataburger eating a burger with...   \n",
       "\n",
       "                                                           post_tokens_cleaned  \\\n",
       "1179055004553900032_twitter  [do not, think, i am, getting, baby, white, tw...   \n",
       "1179063826874032128_twitter  [continue, calling, feminist, right, womxn, ar...   \n",
       "1178793830532956161_twitter                  [nawt, you all, nigger, ignoring]   \n",
       "1179088797964763136_twitter  [user, bit, confused, coz, chinese, people, ac...   \n",
       "1179085312976445440_twitter  [bitch, whataburger, eating, burger, top, bun,...   \n",
       "\n",
       "                             label  \n",
       "1179055004553900032_twitter      0  \n",
       "1179063826874032128_twitter      0  \n",
       "1178793830532956161_twitter      0  \n",
       "1179088797964763136_twitter      1  \n",
       "1179085312976445440_twitter      1  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "df['post_tokens_cleaned'] = df['post_tokens_string'].apply(clean_text)\n",
    "\n",
    "def majority_label(annotations):\n",
    "    labels = [annotation['label'] for annotation in annotations]\n",
    "    counter = Counter(labels)\n",
    "    majority_label = counter.most_common(1)[0][0]\n",
    "    return 0 if majority_label == 'normal' else 1\n",
    "\n",
    "df['label'] = df['annotators'].apply(majority_label)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\"\"\"\n",
    "    use Word2Vec or Sentence2Vec to vectorize the cleaned text data\n",
    " \n",
    "    Paramaters\n",
    "    ---------\n",
    "    cleaned_text: (cleaned) text (tweet) data in our dataframe.\n",
    "    \"\"\"\n",
    "    # When determining how to return the processed data, consider the format your data needs to be in for train_test_split and model training\n",
    "    # Later: Sentence2Vec or Doc2Vec\n",
    "\n",
    "def flatten(l):\n",
    "        return [item for sublist in l for item in sublist]\n",
    "\n",
    "def vectorize_text(cleaned_text, use_word2vec=True):\n",
    "    # Initialize Word2Vec model\n",
    "    model = Word2Vec(cleaned_text, min_count=1)\n",
    "    \n",
    "    # Initialize feature matrix\n",
    "    feature_matrix = []\n",
    "\n",
    "    # Iterate over each sentence in cleaned text\n",
    "    for sentence in cleaned_text:\n",
    "\n",
    "        # Initialize vectors and word count\n",
    "        vectors = []\n",
    "        word_count = len(sentence)\n",
    "\n",
    "        # Generate word vectors for each word in sentence\n",
    "        for word in sentence:\n",
    "             vectors.append(model.wv[word].tolist())\n",
    "\n",
    "        # Initialize averaged vector\n",
    "        averaged_vector = []\n",
    "\n",
    "        # Calculate average vector for each word in sentence\n",
    "        for i in range(0, len(vectors[0])):\n",
    "            count = 0\n",
    "            \n",
    "            for v in vectors:\n",
    "                count += v[i]\n",
    "                \n",
    "            avg = count/word_count\n",
    "            \n",
    "            averaged_vector.append(avg)\n",
    "            \n",
    "        # Append averaged vector to feature matrix\n",
    "        feature_matrix.append(averaged_vector)\n",
    "\n",
    "    # Return feature matrix\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20148 100\n"
     ]
    }
   ],
   "source": [
    "feature_matrix = vectorize_text(df['post_tokens_cleaned'])\n",
    "\n",
    "print(len(feature_matrix), len(feature_matrix[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training: KNN/Random Forest/RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## TO-DO: split our data into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, df[\"label\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using KNN\n",
    "###### Refer to this resource: https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: Determine initial value of parameters (especially K)\n",
    "k = 0\n",
    "\n",
    "## TO-DO: determine the best distance computation metric\n",
    "candidate_metrics = ['euclidean','cosine','manhattan']\n",
    "best_metric = ''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "## TO-DO: fit a KNN Classifier with distance metric and K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO: calculating accuracy\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: optimize K with hyperparameter tuning\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "## TO-DO: Visualize results to determine best K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: repeat training with optimized K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using XGBoost/Random Forest\n",
    "###### Refer to this resource: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: Train model naively, with default parameters\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: Evaluate base model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RandomizedSearchCV.__init__() missing 2 required positional arguments: 'estimator' and 'param_distributions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/arthurzhao/Documents/DAML/HateSpeech/daml-hatespeech-1/getting_started.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arthurzhao/Documents/DAML/HateSpeech/daml-hatespeech-1/getting_started.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomizedSearchCV\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arthurzhao/Documents/DAML/HateSpeech/daml-hatespeech-1/getting_started.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m random_grid \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/arthurzhao/Documents/DAML/HateSpeech/daml-hatespeech-1/getting_started.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m rf_random \u001b[39m=\u001b[39m RandomizedSearchCV()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arthurzhao/Documents/DAML/HateSpeech/daml-hatespeech-1/getting_started.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m## Grid Search\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arthurzhao/Documents/DAML/HateSpeech/daml-hatespeech-1/getting_started.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m GridSearchCV\n",
      "\u001b[0;31mTypeError\u001b[0m: RandomizedSearchCV.__init__() missing 2 required positional arguments: 'estimator' and 'param_distributions'"
     ]
    }
   ],
   "source": [
    "##TO-DO: Hyperparameter Tuning via RandomizedSearch & GridSearch \n",
    "\n",
    "## get current parameters and their values \n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "## Randomized Search \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "random_grid = {}\n",
    "rf_random = RandomizedSearchCV()\n",
    "\n",
    "## Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {}\n",
    "grid_search = GridSearchCV()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using LSTMs\n",
    "###### Refer to this resource: https://www.youtube.com/watch?v=8HyCNIVRbSU&ab_channel=TheA.I.Hacker-MichaelPhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import LSTM, Activation, Dropout, Dense, Embedding\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from keras_preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## veryyyyyy basic architecture. TO-DO: complete and expand\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding())\n",
    "model.add(LSTM())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='',optimizer='', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 0\n",
    "batch_size = 0\n",
    "\n",
    "## TO-DO: determine training parameters and complete call\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: evaluate function \n",
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BERT\n",
    "###### Refer to: http://jalammar.github.io/illustrated-bert/ \n",
    "###### Refer to: https://medium.com/@parthdholakiya180/twitter-hate-detection-using-bert-e7682b2d0a0c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
