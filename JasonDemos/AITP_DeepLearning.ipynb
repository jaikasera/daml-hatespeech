{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Neural Network for Digit Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] accessing MNIST...\")\n",
    "## The MNIST digit classification dataset is popular enough that it's available within the keras package!\n",
    "((trainX, trainY), (testX, testY)) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data into 1D array of pixel values\n",
    "trainX = trainX.reshape((trainX.shape[0], 28 * 28 * 1))\n",
    "testX = testX.reshape((testX.shape[0], 28 * 28 * 1))\n",
    "\n",
    "#normalize between 0 and 1\n",
    "trainX = trainX.astype(\"float32\") / 255.0\n",
    "testX = testX.astype(\"float32\") / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels for each of the 10 digits we want to classify\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seqeuential <== defining a feed-forward neural network; sequentially add layers on top\n",
    "model = Sequential()\n",
    "## Dense layers <== fully connected neural network layer\n",
    "model.add(Dense(256, input_shape=(784,), activation=\"sigmoid\"))\n",
    "model.add(Dense(128, activation=\"sigmoid\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model using SGD\n",
    "print(\"[INFO] training network...\")\n",
    "sgd = SGD(0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "H = model.fit(trainX, trainY, validation_data=(testX, testY),\n",
    "\tepochs=100, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=128)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "\tpredictions.argmax(axis=1),\n",
    "\ttarget_names=[str(x) for x in lb.classes_]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs with IMDb Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Binary Classification! Many-to-One RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
    "                          as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "# shuffle the data, update model parameters based on batch learning \n",
    "batch_size = 32\n",
    "train_dataset = train_dataset.shuffle(10000) \n",
    "train_dataset = train_dataset.batch(batch_size) \n",
    "test_dataset = test_dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## retain at most 1000 words in our vocabulary\n",
    "VOCAB_SIZE = 1000\n",
    "## create an encoder that vectorizes raw text into word IDs.\n",
    "encoder = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE, standardize='lower_and_strip_punctuation')\n",
    "## fit encoder to strings/tokens that are in text; each word in dataset gets a unique ID\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    encoder,\n",
    "    ## embed token IDs into embeddings of size 64  \n",
    "    Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    #a RNN layer that creates feedback loops between input sequences\n",
    "    SimpleRNN(64),\n",
    "    ## Classification Head\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              ## Adam optimizer is another popular optimizer used for minimizing loss\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs with CIFAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "## Convolutional Filter with size 3x3\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "## Max Pooling using 2x2 patch\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "## Repeat \n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final classificaiton head; features extracted from images are flattened into a feed-forward neural network\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "H = model.fit(train_images, train_labels, epochs=10, \n",
    "                    validation_data=(test_images, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AITP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
